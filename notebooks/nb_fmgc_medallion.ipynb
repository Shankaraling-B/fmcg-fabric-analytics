{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f8e12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bronze→Silver→Gold transform, incremental load with watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd431ef",
   "metadata": {},
   "source": [
    "# ================== nb_fmcg_medallion ==================\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from notebookutils import mssparkutils\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "\n",
    "# --------- 0) Environment selection ---------\n",
    "try:\n",
    "    env = mssparkutils.env.getJobTag(\"env\")\n",
    "    if not env:\n",
    "        env = \"dev\"\n",
    "except:\n",
    "    env = \"dev\"\n",
    "print(f\"Environment: {env}\")\n",
    "\n",
    "# --------- 1) Load env config ---------\n",
    "cfg_path = f\"Files/config/{env}.json\"\n",
    "cfg_str  = mssparkutils.fs.head(cfg_path, 5_000_000)\n",
    "cfg      = json.loads(cfg_str)\n",
    "\n",
    "SRC_GLOB = cfg[\"source_glob\"]\n",
    "TBL      = cfg[\"tables\"]\n",
    "DQ_OPTS  = cfg.get(\"dq\", {})\n",
    "JDBC     = cfg.get(\"jdbc\", {\"enabled\": False})\n",
    "\n",
    "print(\"Source:\", SRC_GLOB)\n",
    "\n",
    "# --------- 2) BRONZE (with your VALIDATED transforms) ---------\n",
    "bronze_df = (spark.read.format(\"csv\")\n",
    "             .option(\"header\", True)\n",
    "             .option(\"multiLine\", True)\n",
    "             .option(\"escape\", '\"')\n",
    "             .load(SRC_GLOB))\n",
    "bronze_df = bronze_df.withColumn(\"date\", F.to_date(\"date\", \"M/d/yyyy\"))\n",
    "bronze_df = bronze_df.withColumn(\"price_unit\", bronze_df[\"price_unit\"].cast(T.DoubleType()))\n",
    "bronze_df = bronze_df.withColumn(\"promotion_flag\", bronze_df[\"promotion_flag\"].cast(T.BooleanType()))\n",
    "bronze_df = bronze_df.withColumn(\"region\", F.initcap(F.col(\"region\")))\n",
    "\n",
    "# (Optional) Keep a raw snapshot table; else this is your bronze canonical\n",
    "bronze_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TBL[\"bronze_raw\"])\n",
    "print(f\"Bronze written: {TBL['bronze_raw']}\")\n",
    "\n",
    "# --------- 3) SILVER (incremental + standardization) ---------\n",
    "# Additional standardization: dedupe, strict types for other numeric columns\n",
    "incoming = (bronze_df\n",
    "    .dropDuplicates([\"date\", \"sku\", \"channel\"])\n",
    "    .withColumn(\"units_sold\",      F.col(\"units_sold\").cast(T.LongType()))\n",
    "    .withColumn(\"delivered_qty\",   F.col(\"delivered_qty\").cast(T.LongType()))\n",
    "    .withColumn(\"delivery_days\",   F.col(\"delivery_days\").cast(T.DoubleType()))\n",
    "    .withColumn(\"stock_available\", F.col(\"stock_available\").cast(T.LongType()))\n",
    ")\n",
    "\n",
    "# Watermark read\n",
    "wm_df = spark.table(TBL[\"meta_watermark\"]).filter(F.col(\"table_name\")==TBL[\"silver_clean\"])\n",
    "maxdt = wm_df.select(\"max_date\").head()[\"max_date\"] if wm_df.count() else None\n",
    "\n",
    "df_new = incoming if maxdt is None else incoming.filter(F.col(\"date\") > F.lit(maxdt))\n",
    "rows = df_new.count()\n",
    "\n",
    "if rows > 0:\n",
    "    # Append to Silver\n",
    "    df_new.write.mode(\"append\").format(\"delta\").saveAsTable(TBL[\"silver_clean\"])\n",
    "\n",
    "    # Update watermark & audit with typed DataFrames (no string 'DATE' parsing issues)\n",
    "    stats = df_new.agg(F.min(\"date\").alias(\"min_d\"), F.max(\"date\").alias(\"max_d\")).collect()[0]\n",
    "    min_d, max_d = stats[\"min_d\"], stats[\"max_d\"]\n",
    "\n",
    "    spark.sql(f\"DELETE FROM {TBL['meta_watermark']} WHERE table_name='{TBL['silver_clean']}'\")\n",
    "    spark.createDataFrame([(TBL[\"silver_clean\"], max_d)],\n",
    "                          \"table_name string, max_date date\"\n",
    "    ).write.mode(\"append\").saveAsTable(TBL[\"meta_watermark\"])\n",
    "\n",
    "    spark.createDataFrame(\n",
    "        [(datetime.now(), SRC_GLOB, rows, rows, min_d, max_d)],\n",
    "        \"run_ts timestamp, source_glob string, rows_in long, rows_out long, min_date date, max_date date\"\n",
    "    ).write.mode(\"append\").saveAsTable(TBL[\"meta_audit\"])\n",
    "\n",
    "print(f\"Silver incremental done | rows appended: {rows}\")\n",
    "\n",
    "# --------- 4) DQ (optional but recommended) ---------\n",
    "if DQ_OPTS.get(\"enforce_negative_checks\", True) or DQ_OPTS.get(\"enforce_null_key_checks\", True):\n",
    "    s = spark.table(TBL[\"silver_clean\"])\n",
    "    dq = (s.select(\"date\",\"sku\",\"region\",\"channel\",\"units_sold\",\"delivered_qty\",\"price_unit\",\"stock_available\",\"promotion_flag\")\n",
    "      .withColumn(\"err_negative_units\", (F.col(\"units_sold\") < 0).cast(\"int\"))\n",
    "      .withColumn(\"err_negative_delivered\", (F.col(\"delivered_qty\") < 0).cast(\"int\"))\n",
    "      .withColumn(\"err_null_key\", (\n",
    "            F.col(\"sku\").isNull() | F.col(\"region\").isNull() | F.col(\"price_unit\").isNull()\n",
    "      ).cast(\"int\"))\n",
    "      .withColumn(\"err_delivered_lt_sold\", (F.col(\"delivered_qty\") < F.col(\"units_sold\")).cast(\"int\"))\n",
    "      .withColumn(\"error_flag\",\n",
    "            F.col(\"err_negative_units\")+F.col(\"err_negative_delivered\")+F.col(\"err_null_key\")+F.col(\"err_delivered_lt_sold\"))\n",
    "      .filter(\"error_flag > 0\")\n",
    "    )\n",
    "    dq.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TBL[\"dq_errors\"])\n",
    "    print(f\"DQ table written: {TBL['dq_errors']}\")\n",
    "\n",
    "# --------- 5) GOLD (promo, aggregates, BI fact) ---------\n",
    "# Promo table: promotion_flag is BOOLEAN now -> compare to True\n",
    "promo_df = (spark.table(TBL[\"silver_clean\"])\n",
    "    .filter((F.col(\"promotion_flag\") == F.lit(True)) & (F.col(\"price_unit\") > F.lit(8.0)))\n",
    "    .withColumn(\"effective_price\", F.col(\"price_unit\") * F.lit(0.9))\n",
    ")\n",
    "promo_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TBL[\"silver_promo\"])\n",
    "\n",
    "# Region × SKU aggregation\n",
    "agg_df = (spark.table(TBL[\"silver_clean\"])\n",
    "  .groupBy(\"region\",\"sku\")\n",
    "  .agg(\n",
    "    F.sum(\"units_sold\").alias(\"total_units_sold\"),\n",
    "    F.sum(\"delivered_qty\").alias(\"total_delivered_qty\"),\n",
    "    F.avg(\"delivery_days\").alias(\"avg_delivery_days\"),\n",
    "    (F.sum(\"units_sold\")/F.sum(\"stock_available\")).alias(\"stock_utilization\")\n",
    "  )\n",
    ")\n",
    "agg_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TBL[\"gold_region_sku\"])\n",
    "\n",
    "# Category contribution\n",
    "cat_df = (spark.table(TBL[\"silver_clean\"])\n",
    "  .groupBy(\"segment\",\"category\",\"pack_type\")\n",
    "  .agg(\n",
    "     F.sum(\"units_sold\").alias(\"total_sales_units\"),\n",
    "     F.sum(\"stock_available\").alias(\"total_stock\")\n",
    "  )\n",
    "  .withColumn(\"sell_through_rate\",\n",
    "              F.when(F.col(\"total_stock\")>0, F.col(\"total_sales_units\")/F.col(\"total_stock\"))\n",
    "               .otherwise(F.lit(None)))\n",
    ")\n",
    "cat_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TBL[\"gold_category\"])\n",
    "\n",
    "# Final BI fact (Direct Lake)\n",
    "sales = spark.table(TBL[\"silver_clean\"])\n",
    "promo = spark.table(TBL[\"silver_promo\"])\n",
    "\n",
    "gold_df = (sales.alias(\"s\")\n",
    "  .join(\n",
    "      promo.select(\"date\",\"sku\",\"channel\",\"effective_price\").alias(\"p\"),\n",
    "      on=[\"date\",\"sku\",\"channel\"], how=\"left\"\n",
    "  )\n",
    "  .withColumn(\"unit_price_final\", F.coalesce(F.col(\"p.effective_price\"), F.col(\"s.price_unit\")))\n",
    "  .withColumn(\"line_revenue\", F.col(\"s.units_sold\") * F.col(\"unit_price_final\"))\n",
    "  .groupBy(\"s.date\",\"s.region\",\"s.brand\",\"s.sku\")\n",
    "  .agg(\n",
    "     F.sum(\"s.units_sold\").alias(\"total_units_sold\"),\n",
    "     F.sum(\"line_revenue\").alias(\"total_revenue\"),\n",
    "     (F.sum(\"s.units_sold\")/F.sum(\"s.stock_available\")).alias(\"stock_utilization\"),\n",
    "     F.max(\"s.promotion_flag\").alias(\"promotion_flag\")\n",
    "  )\n",
    ")\n",
    "\n",
    "(gold_df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .format(\"delta\")\n",
    "   .partitionBy(\"date\")\n",
    "   .saveAsTable(TBL[\"gold_bi\"]))\n",
    "print(f\"Gold written: {TBL['gold_bi']}\")\n",
    "# ================== end notebook ==================\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
